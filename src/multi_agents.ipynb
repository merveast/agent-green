{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7276db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import config\n",
    "from datetime import datetime\n",
    "from log_utils import read_log_messages, normalize_template, normalize_template_v1, normalize_template_v2, save_templates, extract_last_template_from_history, extract_template_from_parser_responses, extract_event_templates, extract_last_template_from_history_loose\n",
    "from ollama_utils import start_ollama_server, stop_ollama_server\n",
    "from autogen import register_function\n",
    "from evaluation import evaluate_and_save\n",
    "from autogen.coding import LocalCommandLineCodeExecutor\n",
    "from agent_utils import create_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ca449a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "llm_config = config.LLM_CONFIG\n",
    "task = config.TASK_PROMPT\n",
    "LOG_DIR = config.LOG_DIR\n",
    "RESULT_DIR = config.RESULT_DIR\n",
    "WORK_DIR = config.WORK_DIR\n",
    "os.makedirs(RESULT_DIR, exist_ok=True)\n",
    "DESIGN = \"MA-few\"\n",
    "model = llm_config[\"config_list\"][0][\"model\"].replace(\":\", \"-\")\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "project_name = DESIGN.capitalize()\n",
    "exp_name = f\"{project_name}_{model}_{timestamp}\"\n",
    "\n",
    "input_log_file = \"HDFS_200_sampled.log\"\n",
    "log_path = os.path.join(LOG_DIR, input_log_file)\n",
    "logs = read_log_messages(log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0550cfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create Agents ---\n",
    "user_proxy = create_agent(\"conversable\", \"user_proxy_agent\", llm_config, sys_prompt=\"A human admin.\", description=\"A proxy for human input.\")\n",
    "log_parser = create_agent(\"assistant\", \"log_parser_agent\", llm_config, sys_prompt=config.SYS_MSG_LOG_PARSER_FEW_SHOT, description=\"Analyze the log message in order to determine the corresponding template.\")\n",
    "comparator_refiner = create_agent(\"assistant\", \"comparator_refiner_agent\", llm_config, sys_prompt=config.SYS_MSG_COMPARATOR_REFINER_FEW_SHOT, description=\"Compare templates and refine a final correct version.\")\n",
    "code_executor = create_agent(\"code_executor\", \"code_executor_agent\", description=\"I execute the code provided.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbd7ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Register Drain Function ---\n",
    "from drain_utils import parse_log_lines\n",
    "register_function(\n",
    "    parse_log_lines,\n",
    "    caller=user_proxy,\n",
    "    executor=code_executor,\n",
    "    description=\"Parse the log message with Drain algorithm.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7170a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Energy Measurement ---\n",
    "from codecarbon import EmissionsTracker, OfflineEmissionsTracker\n",
    "#proc = start_ollama_server()\n",
    "time.sleep(5)\n",
    "last_templates = []\n",
    "tracker = OfflineEmissionsTracker(project_name=exp_name, output_dir=RESULT_DIR, country_iso_code=\"CAN\", save_to_file=True)\n",
    "tracker.start()\n",
    "try:\n",
    "    for i, log_message in enumerate(logs):\n",
    "        print(f\"\\n--- Processing log {i+1} ---\")\n",
    "        arguments = {\n",
    "            \"log_line\": log_message,\n",
    "            \"log_format\": r\"<Date> <Time> <Pid> <Level> <Component>: <Content>\",\n",
    "            \"regex\": [r\"blk_-?\\d+\", r\"(\\d+\\.){3}\\d+(:\\d+)?\"],\n",
    "            \"depth\": 4,\n",
    "            \"similarity_threshold\": 0.5,\n",
    "        }\n",
    "        parser_result = user_proxy.initiate_chat(\n",
    "            recipient=log_parser,\n",
    "            message=task + log_message,\n",
    "            max_turns=1,\n",
    "            summary_method=\"last_msg\"\n",
    "        )\n",
    "        executor_result = user_proxy.initiate_chat(\n",
    "            recipient=code_executor,\n",
    "            message={\n",
    "                \"function_call\": {\n",
    "                    \"name\": \"parse_log_lines\",\n",
    "                    \"arguments\": json.dumps(arguments)\n",
    "                }\n",
    "            },\n",
    "            max_turns=1,\n",
    "            summary_method=\"last_msg\"\n",
    "        )\n",
    "        parser_template = parser_result.summary.strip()\n",
    "        executor_template = executor_result.summary.strip()\n",
    "        combined_message = f\"\"\"\n",
    "            Compare and refine the following two templates generated for this log:\n",
    "\n",
    "            Log Message:\n",
    "            {log_message}\n",
    "\n",
    "            Template from Parser Agent:\n",
    "            {parser_template}\n",
    "\n",
    "            Template from Code Executor Agent:\n",
    "            {executor_template}\n",
    "\n",
    "            Return the best refined version.\n",
    "        \"\"\"\n",
    "        refine_result = user_proxy.initiate_chat(\n",
    "            recipient=comparator_refiner,\n",
    "            message=combined_message,\n",
    "            max_turns=1,\n",
    "            summary_method=\"last_msg\"\n",
    "        )\n",
    "        last_template = extract_last_template_from_history(refine_result.chat_history, \"comparator_refiner_agent\")\n",
    "        if last_template:\n",
    "            last_templates.append(last_template)\n",
    "        else:\n",
    "            last_templates.append(\"NONE\")\n",
    "            print(f\"No valid template extracted from log_parser_agent for log {i}\")\n",
    "finally:\n",
    "    emissions = tracker.stop()\n",
    "    print(f\"\\nðŸŒ± Emissions: {emissions} kg CO2\")\n",
    "    stop_ollama_server(proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4351317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Save Templates ---\n",
    "save_templates(last_templates, llm_config, DESIGN, RESULT_DIR)\n",
    "\n",
    "# --- Evaluation ---\n",
    "ground_truth_file = os.path.join(LOG_DIR, \"HDFS_200_sampled_log_structured.csv\")\n",
    "results = evaluate_and_save(normalize_template, last_templates, ground_truth_file, DESIGN, RESULT_DIR)\n",
    "results_v1 = evaluate_and_save(normalize_template_v1, last_templates, ground_truth_file, DESIGN, RESULT_DIR)\n",
    "results_v2 = evaluate_and_save(normalize_template_v2, last_templates, ground_truth_file, DESIGN, RESULT_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
