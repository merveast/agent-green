import os
import config
from pathlib import Path
from autogen import AssistantAgent, ConversableAgent, GroupChat, GroupChatManager, register_function
from autogen.coding import LocalCommandLineCodeExecutor
from datetime import datetime
from typing import List, Dict, Any, Tuple

# --- Generic Agent Creation ---
def create_agent(agent_type, name, llm_config=None, sys_prompt=None, description=None):
    if agent_type == "assistant":
        return AssistantAgent(
            name=name,
            system_message=sys_prompt,
            description=description,
            llm_config=llm_config,
            human_input_mode="NEVER",
        )
    elif agent_type == "conversable":
        return ConversableAgent(
            name=name,
            system_message=sys_prompt,
            description=description,
            llm_config=llm_config,
            human_input_mode="TERMINATE",
        )
    elif agent_type == "code_executor":
        executor = LocalCommandLineCodeExecutor(
            timeout=10,
            work_dir=Path(config.WORK_DIR)
        )
        return ConversableAgent(
            name=name,
            description=description,
            code_execution_config={"executor": executor},
            human_input_mode="NEVER",
            llm_config=False,
        )
    else:
        raise ValueError("Unknown agent type.")

def save_agent_responses(
    responses: List[str],
    llm_config: Dict[str, Any],
    design: str,
    agent_name: str,
    task_name: str = "task",
    output_dir: str = "agent_responses"
) -> str:
    """
    Save full raw responses generated by LLM agents for a task.

    Args:
        responses: List of raw text outputs from an agent.
        llm_config: Dictionary with LLM configuration (expects ["config_list"][0]["model"]).
        design: Name or tag of the experimental design (e.g., "ma-zero").
        agent_name: Agent identifier (e.g., "generator", "critic", "refiner").
        task_name: Name of the task (default: "task").
        output_dir: Directory where responses will be saved.

    Returns:
        str: Path to the file containing the saved raw responses.
    """
    os.makedirs(output_dir, exist_ok=True)
    model_name = llm_config["config_list"][0]["model"].replace(":", "-")
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    filename = f"{output_dir}/{task_name}_{design}_{agent_name}_{model_name}_{timestamp}.txt"

    with open(filename, "w", encoding="utf-8") as f:
        for idx, response in enumerate(responses, start=1):
            f.write(f"--- Response {idx} ---\n")
            f.write(response.strip() + "\n\n")

    print(f"Saved {len(responses)} responses to {filename}")
    return filename
